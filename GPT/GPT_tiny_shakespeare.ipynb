{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read our shakespeare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"datasets\\tinyshakespeare.txt\", \"r\", encoding=\"UTF-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the dataset is loaded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset:  1115394\n",
      "first 50 chars: \n",
      " ''' \n",
      " First Citizen:\n",
      "Before we proceed any further, hear me speak. \n",
      "'''\n",
      "there was a error reading the dataset\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"length of dataset: \", len(text))\n",
    "    print(\"first 60 chars: \\n ''' \\n\", text[:60], \"\\n'''\")\n",
    "except Exception as e:\n",
    "    pass\n",
    "finally:\n",
    "    print(\"there was a error reading the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print list of all the chars and symbols, that are in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All chars and symbols used:  ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "nuber of all chars and symbols used:  65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"All chars and symbols used: \", chars)\n",
    "print(\"nuber of all chars and symbols used: \", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tokenization functions to convert all the characters and symbols from the dataset into something that GPT can process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 1, 35, 53, 56, 50, 42, 2]\n",
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "# Make a character to integer and integer to character dictionary\n",
    "char_to_int = {char: index for index, char in enumerate(chars)}\n",
    "int_to_char = {index: char for index, char in enumerate(chars)}\n",
    "\n",
    "# Function to convert a string to a list of integers\n",
    "def encoder(s):\n",
    "    return [char_to_int[c] for c in s]\n",
    "\n",
    "# Function to convert a list of integers to a string\n",
    "def decoder(l):\n",
    "    return ''.join([int_to_char[i] for i in l])\n",
    "\n",
    "# Test the functions\n",
    "print(encoder(\"Hello World!\"))\n",
    "print(decoder(encoder(\"Hello World!\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the whole dataset, so that the model can read it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proceed any further, hear me speak.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text = encoder(text)\n",
    "encoded_text[:60]\n",
    "# Decode the first 60 characters and symbols that we just decoded to test if it worked correctly\n",
    "decoder(encoded_text[:60]) # the \"\\n\" indicates a new line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing the encoded text in a torch.tensor object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the whole dataset encoded and loaded: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encoded_text, dtype=torch.long)\n",
    "print(\"Is the whole dataset encoded and loaded:\", data.shape[0] == len(text)) # This checks if the number of chars and symbols are the same in the dataset and the torch.tensor object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = int(0.1*len(data))\n",
    "\n",
    "train_data = data[:test_size]\n",
    "test_data = data[test_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a block size by which the data is going to be separated and fed into the model. The \"+1\", because the first element in the list is the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output of this code block should explain the block_size and how the GPT model predicts data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is [18] the target: 47\n",
      "When input is [18, 47] the target: 56\n",
      "When input is [18, 47, 56] the target: 57\n",
      "When input is [18, 47, 56, 57] the target: 58\n",
      "When input is [18, 47, 56, 57, 58] the target: 1\n",
      "When input is [18, 47, 56, 57, 58, 1] the target: 15\n",
      "When input is [18, 47, 56, 57, 58, 1, 15] the target: 47\n",
      "When input is [18, 47, 56, 57, 58, 1, 15, 47] the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"When input is {context.tolist()}, the target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**batch_size** = number of independent sequences that will be processed in parallel <br>\n",
    "**block_size** = number of the maximum context length for predictions <br>\n",
    "The **get_batch** function selects the **batch_size** by **block_size** chunk of data from a random position. Each batch is from its own independent random location.<br>\n",
    "On the bottom of the output you can see that the input is always from **inputs** **x0** to **inputs** **xn** on a **yn**, and the output to this input is on the **xn** matrix on **yn** in the **targets** table.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "    x 0   x 1   x 2   x 3   x 4   x 5   x 6   x 7\n",
      "y0  43 |   1 |  52 |  53 |  40 |  47 |  50 |  47\n",
      "y1  41 |  39 |  52 |  49 |  43 |  56 |   5 |  42\n",
      "y2  46 |  43 |   1 |  15 |  39 |  54 |  47 |  58\n",
      "y3   6 |   1 |  61 |  43 |   1 |  53 |  59 |  45\n",
      "targets:\n",
      "    x 0   x 1   x 2   x 3   x 4   x 5   x 6   x 7\n",
      "y0   1 |  52 |  53 |  40 |  47 |  50 |  47 |  58\n",
      "y1  39 |  52 |  49 |  43 |  56 |   5 |  42 |   1\n",
      "y2  43 |   1 |  15 |  39 |  54 |  47 |  58 |  53\n",
      "y3   1 |  61 |  43 |   1 |  53 |  59 |  45 |  46\n",
      "----\n",
      "When input is [43], the target is: 1\n",
      "When input is [43, 1], the target is: 52\n",
      "When input is [43, 1, 52], the target is: 53\n",
      "When input is [43, 1, 52, 53], the target is: 40\n",
      "When input is [43, 1, 52, 53, 40], the target is: 47\n",
      "When input is [43, 1, 52, 53, 40, 47], the target is: 50\n",
      "When input is [43, 1, 52, 53, 40, 47, 50], the target is: 47\n",
      "When input is [43, 1, 52, 53, 40, 47, 50, 47], the target is: 58\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4 \n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else test_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "def print_table(tensor):\n",
    "    print(\"    \" + \"   \".join([f\"x{col:2}\" for col in range(tensor.shape[1])]))\n",
    "    for row_idx, row in enumerate(tensor):\n",
    "        print(f\"y{row_idx} \" + \" | \".join([f\"{elem:3}\" for elem in row.tolist()]))\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "#print(xb.shape)\n",
    "#print(xb)\n",
    "#print(\"Data tensor as a table:\")\n",
    "print_table(xb)\n",
    "print('targets:')\n",
    "#print(yb.shape)\n",
    "#print(yb)\n",
    "print_table(yb)\n",
    "\n",
    "print('----')\n",
    "\"\"\"\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")\n",
    "\"\"\"\n",
    "b = 0\n",
    "for t in range(block_size): # time dimension\n",
    "    context = xb[b, :t+1]\n",
    "    target = yb[b,t]\n",
    "    print(f\"When input is {context.tolist()}, the target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be the input with which we are going to work in the following code chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[43,  1, 52, 53, 40, 47, 50, 47],\n",
      "        [41, 39, 52, 49, 43, 56,  5, 42],\n",
      "        [46, 43,  1, 15, 39, 54, 47, 58],\n",
      "        [ 6,  1, 61, 43,  1, 53, 59, 45]])\n"
     ]
    }
   ],
   "source": [
    "print(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a BigramLanguageModel object based on a simple Neural Network model.<br>\n",
    "The token_embedding_table creates a vector for each character or symbol, that stores its context to each other character or symbol.<br>\n",
    "Torch arranges this into a Batch by Time by Channel tensor (batch_size = 4, block_size = 8, vocab_size = 65).<br>\n",
    "The loss uses cross_entropy to validate the preditction based on the targets. But the model dosent look at the context of the last character in the input, it just looks at the last character and based on that (the token_embedding_table) predicts the next character.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "loss: 4.488964080810547  If we chose characters on random the loss would be -ln(1/65) or about 4.16\n",
      "\n",
      "Prediction from our model if the user input is a new line character:\n",
      "FYLXQ&\n",
      "GHhKnKX!$yAhK.wAw?OWApHVTZJBHWAqpoVmg.VG&L3VF j'gIsJKGOwlBG'sb!ua\n",
      "ldxtJzMY?oikKwlZDUVHp:Z?qC$\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step (character)\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(\"loss:\", loss.tolist(), \" If we chose characters on random the loss would be -ln(1/65) or about 4.16\")\n",
    "print(\"\\nPrediction from our model if the user input is a new line character:\", end=\"\")\n",
    "print(decoder(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets optimize and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.416093111038208\n",
      "2.1720921993255615\n",
      "2.299400568008423\n",
      "2.4247143268585205\n",
      "2.403860330581665\n",
      "2.3076975345611572\n",
      "2.489482879638672\n",
      "2.4112446308135986\n",
      "2.314846992492676\n",
      "2.359891891479492\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for step in range(10000):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    \n",
    "    #evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    # resetting the optimizer\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # getting new gradients\n",
    "    loss.backward()\n",
    "    # using the new gradients to update the parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 1000 == 0:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New prediction from our model if the user input is a new line character:\n",
      "Toweem count we lyopotconsu ipe tanof us\n",
      "Hanepl too Hour ngo thicly rmye! hid grgacu\n",
      "Har or oungruan\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNew prediction from our model if the user input is a new line character:\", end=\"\")\n",
    "print(decoder(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f70f9ddedb52fef0ee971ecfe5a171ef0058ae3566f8f51b92b01bb1117eedf0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
